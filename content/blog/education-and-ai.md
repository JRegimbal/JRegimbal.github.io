---
title: Thoughts on Education and AI
author: Juliette Regimbal
date: 2024-10-02
tags:
    - opinion
    - education
    - artificial-intelligence
---
Recently, I had a conversation with a few friends who are current or recent PhD students about higher education that surprised me. One of them was adamant that the recent advances in LLMs and generative AI more broadly meant that higher education would be a radically different experience for the next generation of students. What most caught my attention was the suggestion that new technologies would let a student with a secondary education complete the equivalent of a PhD in roughly the time it takes to finish a bachelor's degree now.

This is not an idea I agree with. I've been a teaching assistant for several years at this point and have studied pedagogy beyond the limited TA training offered to us because I want to be a better educator. I care deeply about higher education. I also don't doubt that LLMs and the like will change our education system; I have already redesigned assignments because tools such as ChatGPT make it so easy to produce paragraphs and paragraphs of plausible writing. Changes are necessary and will come, but I don't believe they will (or, at least, should) be such a dramatic shortening of educational programs.

I'm writing this post to explore some of the ideas raised in that conversation. Although I expect what I argue here applies to fields of study outside the STEM umbrella, I'll be focusing on that since my experience lies within engineering education. To start:

## Proposal: LLMs and GenAI will accelerate STEM training and research

I will try my best to explain my friend's "high school to PhD in half the time" idea as I understood it. Generative AI models have been trained on highly technical work in more domains than any single person could ever hope to master. With the ability of LLMs to paraphrase and produce ideas, plans, and procedures from a core idea, students in STEM will be able to perform at the same level as a present graduate of a BSc or BEng program does in only a few months.

The large amount of training data in LLMs will also make it easy for interesting research questions to be identified since literature review would become far easier. Issues with hallucination, in this vision of the future, would be resolved quickly. Studies would still need to be conducted, naturally, but with the literature review and technical implementation preceding them sped up due to AI, and the assistance with writing up the results after, the timeline to reach PhD level outcomes would dramatically reduce.

If the performance of GenAI in new types of tasks continues to improve, then most of what students learn to do at any level of post-secondary education will be easily replicable with an AI assistant. Universities, then, will take these changes into account and reduce the lengths of their programs, since it will be harder than ever to convince prospective students to commit so much of their time unnecessarily.

## Response: AI impacts assessment, not understanding

For now, let's assume that generative AI works or will work the way described in the previous section, despite there being plenty of signs that that is not actually the case. My disagreement is not that this particular technology is unsuitable to accelerate education, so to speak, it's that it ignores key purposes of higher education, especially education focused on research, altogether. STEM programs are not solely intended to develop in their graduates a particular sets of skills to take out into the world (read: workplace).

If this sounds odd to you, consider when the last time was that you faced an issue in your work that perfectly resembled a problem you encountered in class. Real-world situations are far more complex than what could ever be covered (and meaningfully evaluated) in the classroom. In order to transfer what was learned to these scenarios, students must form and refine an understanding of how the world and its problems work. [These understandings](https://www.issmge.org/uploads/publications/3/5/CH002.pdf) empower students to apply skills and knowledge, and recognize when additional learning will be necessary. The goal of education is nothing less than preparing the student to act autonomously and collaborate with others, whether as an engineer, a scientist, or any other member of society. We want people to operate at the top of [Bloom's taxonomy](https://cft.vanderbilt.edu/guides-sub-pages/blooms-taxonomy/) in their regular responsibilities and be self-aware enough to recognize their limits elsewhere.

LLMs do statistical inference on language. ChatGPT doesn't understand, even when it succeeds in tasks that we accept as indicating understanding in humans. If it does succeed in providing correct information (which it may not) or produces a satisfactory procedure to follow (until it backfires), it is incapable of making sense of the problem at hand and must rely on its human operator to inform it of the appropriate context. The risks explode when the operator themself lacks the expertise to take a critical approach to the framing of the problem or the solution provided.

So can AI help students build these kinds of understanding more quickly? Maybe, but learning takes time. Understanding is built through practice under different circumstances, and a student's cognitive capacity won't grow several fold by interacting with GenAI. If it were, we probably would've seen it by now, either through GenAI or a previous information sharing technology (like the one you're using now). Instead, AI is at best a tool that can be used in certain circumstances, like a calculator.

Calculators didn't render math obsolete, but they did require assessments to be redesigned to evaluate students in more meaningful, [authentic](https://www.taylorfrancis.com/chapters/mono/10.4324/9780203818268-2/designing-authentic-assessment-kay-sambell-liz-mcdowell-catherine-montgomery) ways. What does that mean for LLMs? In my experience so far, it means that written documents will become much more difficult to meaningfully evaluate. In the course I assist with, my supervisor, fellow TAs and I have already decided to move towards handwritten documents for assignments requiring quick assessments. For more detailed project reports, we've allocated additional time to evaluations of the texts themselves, and also included opportunities for the student to discuss their work with their peers, under the supervision of a TA. What we hope to do is give ourselves the time to ensure that reports actually are coherent and complete reflections of their work as demonstrated through project artifacts and interactive means of communication. This will probably need to be refined, of course, over several semesters to reach a state that is sustainable for educators and students alike. I don't see how or why the procedure we settle on would be on a significantly different timeline than what we have now.

Going even further, I wonder if evaluation in the future will move towards less "adversarial" approaches, such [ungrading](https://ctlt.ubc.ca/2022/03/31/edubytes-ungrading/), in order to focus more on students achieving learning outcomes. Of course, there will be a tension so long as "failure", whether actual or perceived, has material consequences on a student's life. If a slightly worse mark arbitrarily closes off future career paths, or a course retaken puts a student further into debt between tuition and the cost of living, it's easy to understand why a student might feel pressure to "take shortcuts". This is especially true as more credentials and experience are required for entry-level work. These questions and concerns go well past the scope of the blog post, however, though I am interested in seeing where the discourse goes in the next few years.

Did you think this was interesting? Do you think I'm completely wrong? Get in touch! Send me an email at {firstname}@{fullname}.ca!
